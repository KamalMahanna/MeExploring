{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee6823cd",
   "metadata": {},
   "source": [
    "### Length Based"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14007dcb",
   "metadata": {},
   "source": [
    "simple on the basis of chunksize and overlapping, it split the text into chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dfaa8e7",
   "metadata": {},
   "source": [
    "[https://chunkviz.up.railway.app](https://chunkviz.up.railway.app)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "596a5245",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = \"\"\"One of the most important things I didn't understand about the world when I was a child is the degree to which the returns for performance are superlinear.\n",
    "\n",
    "Teachers and coaches implicitly told us the returns were linear. \"You get out,\" I heard a thousand times, \"what you put in.\" They meant well, but this is rarely true. If your product is only half as good as your competitor's, you don't get half as many customers. You get no customers, and you go out of business.\n",
    "\n",
    "It's obviously true that the returns for performance are superlinear in business. Some think this is a flaw of capitalism, and that if we changed the rules it would stop being true. But superlinear returns for performance are a feature of the world, not an artifact of rules we've invented. We see the same pattern in fame, power, military victories, knowledge, and even benefit to humanity. In all of these, the rich get richer. [1]\n",
    "\n",
    "You can't understand the world without understanding the concept of superlinear returns. And if you're ambitious you definitely should, because this will be the wave you surf on.\n",
    "\n",
    "It may seem as if there are a lot of different situations with superlinear returns, but as far as I can tell they reduce to two fundamental causes: exponential growth and thresholds.\n",
    "\n",
    "The most obvious case of superlinear returns is when you're working on something that grows exponentially. For example, growing bacterial cultures. When they grow at all, they grow exponentially. But they're tricky to grow. Which means the difference in outcome between someone who's adept at it and someone who's not is very great.\n",
    "\n",
    "Startups can also grow exponentially, and we see the same pattern there. Some manage to achieve high growth rates. Most don't. And as a result you get qualitatively different outcomes: the companies with high growth rates tend to become immensely valuable, while the ones with lower growth rates may not even survive.\n",
    "\n",
    "Y Combinator encourages founders to focus on growth rate rather than absolute numbers. It prevents them from being discouraged early on, when the absolute numbers are still low. It also helps them decide what to focus on: you can use growth rate as a compass to tell you how to evolve the company. But the main advantage is that by focusing on growth rate you tend to get something that grows exponentially.\n",
    "\n",
    "YC doesn't explicitly tell founders that with growth rate \"you get out what you put in,\" but it's not far from the truth. And if growth rate were proportional to performance, then the reward for performance p over time t would be proportional to pt.\n",
    "\n",
    "Even after decades of thinking about this, I find that sentence startling.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44530397",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"One of the most important things I didn't understand about the world when I was a child is the\",\n",
       " 'was a child is the degree to which the returns for performance are superlinear.',\n",
       " 'Teachers and coaches implicitly told us the returns were linear. \"You get out,\" I heard a thousand',\n",
       " 'I heard a thousand times, \"what you put in.\" They meant well, but this is rarely true. If your',\n",
       " \"true. If your product is only half as good as your competitor's, you don't get half as many\",\n",
       " 'get half as many customers. You get no customers, and you go out of business.',\n",
       " \"It's obviously true that the returns for performance are superlinear in business. Some think this\",\n",
       " 'Some think this is a flaw of capitalism, and that if we changed the rules it would stop being true.',\n",
       " 'stop being true. But superlinear returns for performance are a feature of the world, not an',\n",
       " \"the world, not an artifact of rules we've invented. We see the same pattern in fame, power,\",\n",
       " 'in fame, power, military victories, knowledge, and even benefit to humanity. In all of these, the',\n",
       " 'all of these, the rich get richer. [1]',\n",
       " \"You can't understand the world without understanding the concept of superlinear returns. And if\",\n",
       " \"returns. And if you're ambitious you definitely should, because this will be the wave you surf on.\",\n",
       " 'It may seem as if there are a lot of different situations with superlinear returns, but as far as I',\n",
       " 'but as far as I can tell they reduce to two fundamental causes: exponential growth and thresholds.',\n",
       " \"The most obvious case of superlinear returns is when you're working on something that grows\",\n",
       " 'that grows exponentially. For example, growing bacterial cultures. When they grow at all, they grow',\n",
       " \"at all, they grow exponentially. But they're tricky to grow. Which means the difference in outcome\",\n",
       " \"in outcome between someone who's adept at it and someone who's not is very great.\",\n",
       " 'Startups can also grow exponentially, and we see the same pattern there. Some manage to achieve',\n",
       " \"manage to achieve high growth rates. Most don't. And as a result you get qualitatively different\",\n",
       " 'different outcomes: the companies with high growth rates tend to become immensely valuable, while',\n",
       " 'valuable, while the ones with lower growth rates may not even survive.',\n",
       " 'Y Combinator encourages founders to focus on growth rate rather than absolute numbers. It prevents',\n",
       " 'It prevents them from being discouraged early on, when the absolute numbers are still low. It also',\n",
       " 'still low. It also helps them decide what to focus on: you can use growth rate as a compass to tell',\n",
       " 'a compass to tell you how to evolve the company. But the main advantage is that by focusing on',\n",
       " 'that by focusing on growth rate you tend to get something that grows exponentially.',\n",
       " 'YC doesn\\'t explicitly tell founders that with growth rate \"you get out what you put in,\" but it\\'s',\n",
       " 'put in,\" but it\\'s not far from the truth. And if growth rate were proportional to performance, then',\n",
       " 'performance, then the reward for performance p over time t would be proportional to pt.',\n",
       " 'Even after decades of thinking about this, I find that sentence startling.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spliter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=20)\n",
    "spliter.split_text(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37784951",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7a06ae2d",
   "metadata": {},
   "source": [
    "### Text-Structure Splitter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681bfc64",
   "metadata": {},
   "source": [
    "based on structure of characters, it spilt texts into hirarchy like forst make chunk on basis of paragraph, then split on basis of sentences, then split on basis of words, lastly split on basis of characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0ea2171",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "spliter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=20,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31a0495a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"One of the most important things I didn't understand about the world when I was a child is the\",\n",
       " 'was a child is the degree to which the returns for performance are superlinear.',\n",
       " 'Teachers and coaches implicitly told us the returns were linear. \"You get out,\" I heard a thousand',\n",
       " 'I heard a thousand times, \"what you put in.\" They meant well, but this is rarely true. If your',\n",
       " \"true. If your product is only half as good as your competitor's, you don't get half as many\",\n",
       " 'get half as many customers. You get no customers, and you go out of business.',\n",
       " \"It's obviously true that the returns for performance are superlinear in business. Some think this\",\n",
       " 'Some think this is a flaw of capitalism, and that if we changed the rules it would stop being true.',\n",
       " 'stop being true. But superlinear returns for performance are a feature of the world, not an',\n",
       " \"the world, not an artifact of rules we've invented. We see the same pattern in fame, power,\",\n",
       " 'in fame, power, military victories, knowledge, and even benefit to humanity. In all of these, the',\n",
       " 'all of these, the rich get richer. [1]',\n",
       " \"You can't understand the world without understanding the concept of superlinear returns. And if\",\n",
       " \"returns. And if you're ambitious you definitely should, because this will be the wave you surf on.\",\n",
       " 'It may seem as if there are a lot of different situations with superlinear returns, but as far as I',\n",
       " 'but as far as I can tell they reduce to two fundamental causes: exponential growth and thresholds.',\n",
       " \"The most obvious case of superlinear returns is when you're working on something that grows\",\n",
       " 'that grows exponentially. For example, growing bacterial cultures. When they grow at all, they grow',\n",
       " \"at all, they grow exponentially. But they're tricky to grow. Which means the difference in outcome\",\n",
       " \"in outcome between someone who's adept at it and someone who's not is very great.\",\n",
       " 'Startups can also grow exponentially, and we see the same pattern there. Some manage to achieve',\n",
       " \"manage to achieve high growth rates. Most don't. And as a result you get qualitatively different\",\n",
       " 'different outcomes: the companies with high growth rates tend to become immensely valuable, while',\n",
       " 'valuable, while the ones with lower growth rates may not even survive.',\n",
       " 'Y Combinator encourages founders to focus on growth rate rather than absolute numbers. It prevents',\n",
       " 'It prevents them from being discouraged early on, when the absolute numbers are still low. It also',\n",
       " 'still low. It also helps them decide what to focus on: you can use growth rate as a compass to tell',\n",
       " 'a compass to tell you how to evolve the company. But the main advantage is that by focusing on',\n",
       " 'that by focusing on growth rate you tend to get something that grows exponentially.',\n",
       " 'YC doesn\\'t explicitly tell founders that with growth rate \"you get out what you put in,\" but it\\'s',\n",
       " 'put in,\" but it\\'s not far from the truth. And if growth rate were proportional to performance, then',\n",
       " 'performance, then the reward for performance p over time t would be proportional to pt.',\n",
       " 'Even after decades of thinking about this, I find that sentence startling.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spliter.split_text(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b5a1ca11",
   "metadata": {},
   "source": [
    "### Document-Structured based"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157b3134",
   "metadata": {},
   "source": [
    "Some time the texts are not pain texts rather it might be a code snippet or a document. In such cases, we need to split the text based on different document structure. like based on \\nclass, \\ndef etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = \"\"\"\n",
    "class Student:\n",
    "    def __init__(self, name, age):\n",
    "        self.name = name\n",
    "        self.age = age\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"Student(name={self.name}, age={self.age})\"\n",
    "        \"\"\"\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a4a9c84e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['class Student:\\n    def __init__(self, name, age):\\n        self.name = name\\n        self.age = age',\n",
       " 'def __str__(self):\\n        return f\"Student(name={self.name}, age={self.age})\"']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, Language\n",
    "\n",
    "spliter = RecursiveCharacterTextSplitter.from_language(\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=20,\n",
    "    language=Language.PYTHON,\n",
    ")\n",
    "\n",
    "texts = spliter.split_text(texts)\n",
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42576814",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ce25fe0b",
   "metadata": {},
   "source": [
    "### Semantic Meaning Based"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2916b7a5",
   "metadata": {},
   "source": [
    "check similarity of each sentence with next sentence and it will group/chunk them based on similarity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a08a14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "\n",
    "texts = \"\"\"\n",
    "ne of the most important things I didn't understand about the world when I was a child is the degree to which the returns for performance are superlinear.\n",
    "\n",
    "Teachers and coaches implicitly told us the returns were linear. \"You get out,\" I heard a thousand times, \"what you put in.\" They meant well, but this is rarely true. If your product is only half as good as your competitor's, you don't get half as many customers. You get no customers, and you go out of business.\n",
    "\n",
    "It's obviously true that the returns for performance are superlinear in business. Some think this is a flaw of capitalism, and that if we changed the rules it would stop being true. But superlinear returns for performance are a feature of the world, not an artifact of rules we've invented. We see the same pattern in fame, power, military victories, knowledge, and even benefit to humanity. In all of these, the rich get richer. [1]\n",
    "\n",
    "You can't understand the world without understanding the concept of superlinear returns. And if you're ambitious you definitely should, because this will be the wave you surf on.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d7779575",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-19 09:52:00.123300: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-05-19 09:52:00.249711: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1747628520.304247   20386 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1747628520.318299   20386 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1747628520.423261   20386 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747628520.423289   20386 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747628520.423292   20386 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747628520.423293   20386 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-19 09:52:00.435431: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Some weights of the model checkpoint at NovaSearch/jasper_en_vision_language_v1 were not used when initializing JasperVL: ['vision_model.vision_model.embeddings.patch_embedding.bias', 'vision_model.vision_model.embeddings.patch_embedding.weight', 'vision_model.vision_model.embeddings.position_embedding.weight', 'vision_model.vision_model.encoder.layers.0.layer_norm1.bias', 'vision_model.vision_model.encoder.layers.0.layer_norm1.weight', 'vision_model.vision_model.encoder.layers.0.layer_norm2.bias', 'vision_model.vision_model.encoder.layers.0.layer_norm2.weight', 'vision_model.vision_model.encoder.layers.0.mlp.fc1.bias', 'vision_model.vision_model.encoder.layers.0.mlp.fc1.weight', 'vision_model.vision_model.encoder.layers.0.mlp.fc2.bias', 'vision_model.vision_model.encoder.layers.0.mlp.fc2.weight', 'vision_model.vision_model.encoder.layers.0.self_attn.k_proj.bias', 'vision_model.vision_model.encoder.layers.0.self_attn.k_proj.weight', 'vision_model.vision_model.encoder.layers.0.self_attn.out_proj.bias', 'vision_model.vision_model.encoder.layers.0.self_attn.out_proj.weight', 'vision_model.vision_model.encoder.layers.0.self_attn.q_proj.bias', 'vision_model.vision_model.encoder.layers.0.self_attn.q_proj.weight', 'vision_model.vision_model.encoder.layers.0.self_attn.v_proj.bias', 'vision_model.vision_model.encoder.layers.0.self_attn.v_proj.weight', 'vision_model.vision_model.encoder.layers.1.layer_norm1.bias', 'vision_model.vision_model.encoder.layers.1.layer_norm1.weight', 'vision_model.vision_model.encoder.layers.1.layer_norm2.bias', 'vision_model.vision_model.encoder.layers.1.layer_norm2.weight', 'vision_model.vision_model.encoder.layers.1.mlp.fc1.bias', 'vision_model.vision_model.encoder.layers.1.mlp.fc1.weight', 'vision_model.vision_model.encoder.layers.1.mlp.fc2.bias', 'vision_model.vision_model.encoder.layers.1.mlp.fc2.weight', 'vision_model.vision_model.encoder.layers.1.self_attn.k_proj.bias', 'vision_model.vision_model.encoder.layers.1.self_attn.k_proj.weight', 'vision_model.vision_model.encoder.layers.1.self_attn.out_proj.bias', 'vision_model.vision_model.encoder.layers.1.self_attn.out_proj.weight', 'vision_model.vision_model.encoder.layers.1.self_attn.q_proj.bias', 'vision_model.vision_model.encoder.layers.1.self_attn.q_proj.weight', 'vision_model.vision_model.encoder.layers.1.self_attn.v_proj.bias', 'vision_model.vision_model.encoder.layers.1.self_attn.v_proj.weight', 'vision_model.vision_model.encoder.layers.10.layer_norm1.bias', 'vision_model.vision_model.encoder.layers.10.layer_norm1.weight', 'vision_model.vision_model.encoder.layers.10.layer_norm2.bias', 'vision_model.vision_model.encoder.layers.10.layer_norm2.weight', 'vision_model.vision_model.encoder.layers.10.mlp.fc1.bias', 'vision_model.vision_model.encoder.layers.10.mlp.fc1.weight', 'vision_model.vision_model.encoder.layers.10.mlp.fc2.bias', 'vision_model.vision_model.encoder.layers.10.mlp.fc2.weight', 'vision_model.vision_model.encoder.layers.10.self_attn.k_proj.bias', 'vision_model.vision_model.encoder.layers.10.self_attn.k_proj.weight', 'vision_model.vision_model.encoder.layers.10.self_attn.out_proj.bias', 'vision_model.vision_model.encoder.layers.10.self_attn.out_proj.weight', 'vision_model.vision_model.encoder.layers.10.self_attn.q_proj.bias', 'vision_model.vision_model.encoder.layers.10.self_attn.q_proj.weight', 'vision_model.vision_model.encoder.layers.10.self_attn.v_proj.bias', 'vision_model.vision_model.encoder.layers.10.self_attn.v_proj.weight', 'vision_model.vision_model.encoder.layers.11.layer_norm1.bias', 'vision_model.vision_model.encoder.layers.11.layer_norm1.weight', 'vision_model.vision_model.encoder.layers.11.layer_norm2.bias', 'vision_model.vision_model.encoder.layers.11.layer_norm2.weight', 'vision_model.vision_model.encoder.layers.11.mlp.fc1.bias', 'vision_model.vision_model.encoder.layers.11.mlp.fc1.weight', 'vision_model.vision_model.encoder.layers.11.mlp.fc2.bias', 'vision_model.vision_model.encoder.layers.11.mlp.fc2.weight', 'vision_model.vision_model.encoder.layers.11.self_attn.k_proj.bias', 'vision_model.vision_model.encoder.layers.11.self_attn.k_proj.weight', 'vision_model.vision_model.encoder.layers.11.self_attn.out_proj.bias', 'vision_model.vision_model.encoder.layers.11.self_attn.out_proj.weight', 'vision_model.vision_model.encoder.layers.11.self_attn.q_proj.bias', 'vision_model.vision_model.encoder.layers.11.self_attn.q_proj.weight', 'vision_model.vision_model.encoder.layers.11.self_attn.v_proj.bias', 'vision_model.vision_model.encoder.layers.11.self_attn.v_proj.weight', 'vision_model.vision_model.encoder.layers.12.layer_norm1.bias', 'vision_model.vision_model.encoder.layers.12.layer_norm1.weight', 'vision_model.vision_model.encoder.layers.12.layer_norm2.bias', 'vision_model.vision_model.encoder.layers.12.layer_norm2.weight', 'vision_model.vision_model.encoder.layers.12.mlp.fc1.bias', 'vision_model.vision_model.encoder.layers.12.mlp.fc1.weight', 'vision_model.vision_model.encoder.layers.12.mlp.fc2.bias', 'vision_model.vision_model.encoder.layers.12.mlp.fc2.weight', 'vision_model.vision_model.encoder.layers.12.self_attn.k_proj.bias', 'vision_model.vision_model.encoder.layers.12.self_attn.k_proj.weight', 'vision_model.vision_model.encoder.layers.12.self_attn.out_proj.bias', 'vision_model.vision_model.encoder.layers.12.self_attn.out_proj.weight', 'vision_model.vision_model.encoder.layers.12.self_attn.q_proj.bias', 'vision_model.vision_model.encoder.layers.12.self_attn.q_proj.weight', 'vision_model.vision_model.encoder.layers.12.self_attn.v_proj.bias', 'vision_model.vision_model.encoder.layers.12.self_attn.v_proj.weight', 'vision_model.vision_model.encoder.layers.13.layer_norm1.bias', 'vision_model.vision_model.encoder.layers.13.layer_norm1.weight', 'vision_model.vision_model.encoder.layers.13.layer_norm2.bias', 'vision_model.vision_model.encoder.layers.13.layer_norm2.weight', 'vision_model.vision_model.encoder.layers.13.mlp.fc1.bias', 'vision_model.vision_model.encoder.layers.13.mlp.fc1.weight', 'vision_model.vision_model.encoder.layers.13.mlp.fc2.bias', 'vision_model.vision_model.encoder.layers.13.mlp.fc2.weight', 'vision_model.vision_model.encoder.layers.13.self_attn.k_proj.bias', 'vision_model.vision_model.encoder.layers.13.self_attn.k_proj.weight', 'vision_model.vision_model.encoder.layers.13.self_attn.out_proj.bias', 'vision_model.vision_model.encoder.layers.13.self_attn.out_proj.weight', 'vision_model.vision_model.encoder.layers.13.self_attn.q_proj.bias', 'vision_model.vision_model.encoder.layers.13.self_attn.q_proj.weight', 'vision_model.vision_model.encoder.layers.13.self_attn.v_proj.bias', 'vision_model.vision_model.encoder.layers.13.self_attn.v_proj.weight', 'vision_model.vision_model.encoder.layers.14.layer_norm1.bias', 'vision_model.vision_model.encoder.layers.14.layer_norm1.weight', 'vision_model.vision_model.encoder.layers.14.layer_norm2.bias', 'vision_model.vision_model.encoder.layers.14.layer_norm2.weight', 'vision_model.vision_model.encoder.layers.14.mlp.fc1.bias', 'vision_model.vision_model.encoder.layers.14.mlp.fc1.weight', 'vision_model.vision_model.encoder.layers.14.mlp.fc2.bias', 'vision_model.vision_model.encoder.layers.14.mlp.fc2.weight', 'vision_model.vision_model.encoder.layers.14.self_attn.k_proj.bias', 'vision_model.vision_model.encoder.layers.14.self_attn.k_proj.weight', 'vision_model.vision_model.encoder.layers.14.self_attn.out_proj.bias', 'vision_model.vision_model.encoder.layers.14.self_attn.out_proj.weight', 'vision_model.vision_model.encoder.layers.14.self_attn.q_proj.bias', 'vision_model.vision_model.encoder.layers.14.self_attn.q_proj.weight', 'vision_model.vision_model.encoder.layers.14.self_attn.v_proj.bias', 'vision_model.vision_model.encoder.layers.14.self_attn.v_proj.weight', 'vision_model.vision_model.encoder.layers.15.layer_norm1.bias', 'vision_model.vision_model.encoder.layers.15.layer_norm1.weight', 'vision_model.vision_model.encoder.layers.15.layer_norm2.bias', 'vision_model.vision_model.encoder.layers.15.layer_norm2.weight', 'vision_model.vision_model.encoder.layers.15.mlp.fc1.bias', 'vision_model.vision_model.encoder.layers.15.mlp.fc1.weight', 'vision_model.vision_model.encoder.layers.15.mlp.fc2.bias', 'vision_model.vision_model.encoder.layers.15.mlp.fc2.weight', 'vision_model.vision_model.encoder.layers.15.self_attn.k_proj.bias', 'vision_model.vision_model.encoder.layers.15.self_attn.k_proj.weight', 'vision_model.vision_model.encoder.layers.15.self_attn.out_proj.bias', 'vision_model.vision_model.encoder.layers.15.self_attn.out_proj.weight', 'vision_model.vision_model.encoder.layers.15.self_attn.q_proj.bias', 'vision_model.vision_model.encoder.layers.15.self_attn.q_proj.weight', 'vision_model.vision_model.encoder.layers.15.self_attn.v_proj.bias', 'vision_model.vision_model.encoder.layers.15.self_attn.v_proj.weight', 'vision_model.vision_model.encoder.layers.16.layer_norm1.bias', 'vision_model.vision_model.encoder.layers.16.layer_norm1.weight', 'vision_model.vision_model.encoder.layers.16.layer_norm2.bias', 'vision_model.vision_model.encoder.layers.16.layer_norm2.weight', 'vision_model.vision_model.encoder.layers.16.mlp.fc1.bias', 'vision_model.vision_model.encoder.layers.16.mlp.fc1.weight', 'vision_model.vision_model.encoder.layers.16.mlp.fc2.bias', 'vision_model.vision_model.encoder.layers.16.mlp.fc2.weight', 'vision_model.vision_model.encoder.layers.16.self_attn.k_proj.bias', 'vision_model.vision_model.encoder.layers.16.self_attn.k_proj.weight', 'vision_model.vision_model.encoder.layers.16.self_attn.out_proj.bias', 'vision_model.vision_model.encoder.layers.16.self_attn.out_proj.weight', 'vision_model.vision_model.encoder.layers.16.self_attn.q_proj.bias', 'vision_model.vision_model.encoder.layers.16.self_attn.q_proj.weight', 'vision_model.vision_model.encoder.layers.16.self_attn.v_proj.bias', 'vision_model.vision_model.encoder.layers.16.self_attn.v_proj.weight', 'vision_model.vision_model.encoder.layers.17.layer_norm1.bias', 'vision_model.vision_model.encoder.layers.17.layer_norm1.weight', 'vision_model.vision_model.encoder.layers.17.layer_norm2.bias', 'vision_model.vision_model.encoder.layers.17.layer_norm2.weight', 'vision_model.vision_model.encoder.layers.17.mlp.fc1.bias', 'vision_model.vision_model.encoder.layers.17.mlp.fc1.weight', 'vision_model.vision_model.encoder.layers.17.mlp.fc2.bias', 'vision_model.vision_model.encoder.layers.17.mlp.fc2.weight', 'vision_model.vision_model.encoder.layers.17.self_attn.k_proj.bias', 'vision_model.vision_model.encoder.layers.17.self_attn.k_proj.weight', 'vision_model.vision_model.encoder.layers.17.self_attn.out_proj.bias', 'vision_model.vision_model.encoder.layers.17.self_attn.out_proj.weight', 'vision_model.vision_model.encoder.layers.17.self_attn.q_proj.bias', 'vision_model.vision_model.encoder.layers.17.self_attn.q_proj.weight', 'vision_model.vision_model.encoder.layers.17.self_attn.v_proj.bias', 'vision_model.vision_model.encoder.layers.17.self_attn.v_proj.weight', 'vision_model.vision_model.encoder.layers.18.layer_norm1.bias', 'vision_model.vision_model.encoder.layers.18.layer_norm1.weight', 'vision_model.vision_model.encoder.layers.18.layer_norm2.bias', 'vision_model.vision_model.encoder.layers.18.layer_norm2.weight', 'vision_model.vision_model.encoder.layers.18.mlp.fc1.bias', 'vision_model.vision_model.encoder.layers.18.mlp.fc1.weight', 'vision_model.vision_model.encoder.layers.18.mlp.fc2.bias', 'vision_model.vision_model.encoder.layers.18.mlp.fc2.weight', 'vision_model.vision_model.encoder.layers.18.self_attn.k_proj.bias', 'vision_model.vision_model.encoder.layers.18.self_attn.k_proj.weight', 'vision_model.vision_model.encoder.layers.18.self_attn.out_proj.bias', 'vision_model.vision_model.encoder.layers.18.self_attn.out_proj.weight', 'vision_model.vision_model.encoder.layers.18.self_attn.q_proj.bias', 'vision_model.vision_model.encoder.layers.18.self_attn.q_proj.weight', 'vision_model.vision_model.encoder.layers.18.self_attn.v_proj.bias', 'vision_model.vision_model.encoder.layers.18.self_attn.v_proj.weight', 'vision_model.vision_model.encoder.layers.19.layer_norm1.bias', 'vision_model.vision_model.encoder.layers.19.layer_norm1.weight', 'vision_model.vision_model.encoder.layers.19.layer_norm2.bias', 'vision_model.vision_model.encoder.layers.19.layer_norm2.weight', 'vision_model.vision_model.encoder.layers.19.mlp.fc1.bias', 'vision_model.vision_model.encoder.layers.19.mlp.fc1.weight', 'vision_model.vision_model.encoder.layers.19.mlp.fc2.bias', 'vision_model.vision_model.encoder.layers.19.mlp.fc2.weight', 'vision_model.vision_model.encoder.layers.19.self_attn.k_proj.bias', 'vision_model.vision_model.encoder.layers.19.self_attn.k_proj.weight', 'vision_model.vision_model.encoder.layers.19.self_attn.out_proj.bias', 'vision_model.vision_model.encoder.layers.19.self_attn.out_proj.weight', 'vision_model.vision_model.encoder.layers.19.self_attn.q_proj.bias', 'vision_model.vision_model.encoder.layers.19.self_attn.q_proj.weight', 'vision_model.vision_model.encoder.layers.19.self_attn.v_proj.bias', 'vision_model.vision_model.encoder.layers.19.self_attn.v_proj.weight', 'vision_model.vision_model.encoder.layers.2.layer_norm1.bias', 'vision_model.vision_model.encoder.layers.2.layer_norm1.weight', 'vision_model.vision_model.encoder.layers.2.layer_norm2.bias', 'vision_model.vision_model.encoder.layers.2.layer_norm2.weight', 'vision_model.vision_model.encoder.layers.2.mlp.fc1.bias', 'vision_model.vision_model.encoder.layers.2.mlp.fc1.weight', 'vision_model.vision_model.encoder.layers.2.mlp.fc2.bias', 'vision_model.vision_model.encoder.layers.2.mlp.fc2.weight', 'vision_model.vision_model.encoder.layers.2.self_attn.k_proj.bias', 'vision_model.vision_model.encoder.layers.2.self_attn.k_proj.weight', 'vision_model.vision_model.encoder.layers.2.self_attn.out_proj.bias', 'vision_model.vision_model.encoder.layers.2.self_attn.out_proj.weight', 'vision_model.vision_model.encoder.layers.2.self_attn.q_proj.bias', 'vision_model.vision_model.encoder.layers.2.self_attn.q_proj.weight', 'vision_model.vision_model.encoder.layers.2.self_attn.v_proj.bias', 'vision_model.vision_model.encoder.layers.2.self_attn.v_proj.weight', 'vision_model.vision_model.encoder.layers.20.layer_norm1.bias', 'vision_model.vision_model.encoder.layers.20.layer_norm1.weight', 'vision_model.vision_model.encoder.layers.20.layer_norm2.bias', 'vision_model.vision_model.encoder.layers.20.layer_norm2.weight', 'vision_model.vision_model.encoder.layers.20.mlp.fc1.bias', 'vision_model.vision_model.encoder.layers.20.mlp.fc1.weight', 'vision_model.vision_model.encoder.layers.20.mlp.fc2.bias', 'vision_model.vision_model.encoder.layers.20.mlp.fc2.weight', 'vision_model.vision_model.encoder.layers.20.self_attn.k_proj.bias', 'vision_model.vision_model.encoder.layers.20.self_attn.k_proj.weight', 'vision_model.vision_model.encoder.layers.20.self_attn.out_proj.bias', 'vision_model.vision_model.encoder.layers.20.self_attn.out_proj.weight', 'vision_model.vision_model.encoder.layers.20.self_attn.q_proj.bias', 'vision_model.vision_model.encoder.layers.20.self_attn.q_proj.weight', 'vision_model.vision_model.encoder.layers.20.self_attn.v_proj.bias', 'vision_model.vision_model.encoder.layers.20.self_attn.v_proj.weight', 'vision_model.vision_model.encoder.layers.21.layer_norm1.bias', 'vision_model.vision_model.encoder.layers.21.layer_norm1.weight', 'vision_model.vision_model.encoder.layers.21.layer_norm2.bias', 'vision_model.vision_model.encoder.layers.21.layer_norm2.weight', 'vision_model.vision_model.encoder.layers.21.mlp.fc1.bias', 'vision_model.vision_model.encoder.layers.21.mlp.fc1.weight', 'vision_model.vision_model.encoder.layers.21.mlp.fc2.bias', 'vision_model.vision_model.encoder.layers.21.mlp.fc2.weight', 'vision_model.vision_model.encoder.layers.21.self_attn.k_proj.bias', 'vision_model.vision_model.encoder.layers.21.self_attn.k_proj.weight', 'vision_model.vision_model.encoder.layers.21.self_attn.out_proj.bias', 'vision_model.vision_model.encoder.layers.21.self_attn.out_proj.weight', 'vision_model.vision_model.encoder.layers.21.self_attn.q_proj.bias', 'vision_model.vision_model.encoder.layers.21.self_attn.q_proj.weight', 'vision_model.vision_model.encoder.layers.21.self_attn.v_proj.bias', 'vision_model.vision_model.encoder.layers.21.self_attn.v_proj.weight', 'vision_model.vision_model.encoder.layers.22.layer_norm1.bias', 'vision_model.vision_model.encoder.layers.22.layer_norm1.weight', 'vision_model.vision_model.encoder.layers.22.layer_norm2.bias', 'vision_model.vision_model.encoder.layers.22.layer_norm2.weight', 'vision_model.vision_model.encoder.layers.22.mlp.fc1.bias', 'vision_model.vision_model.encoder.layers.22.mlp.fc1.weight', 'vision_model.vision_model.encoder.layers.22.mlp.fc2.bias', 'vision_model.vision_model.encoder.layers.22.mlp.fc2.weight', 'vision_model.vision_model.encoder.layers.22.self_attn.k_proj.bias', 'vision_model.vision_model.encoder.layers.22.self_attn.k_proj.weight', 'vision_model.vision_model.encoder.layers.22.self_attn.out_proj.bias', 'vision_model.vision_model.encoder.layers.22.self_attn.out_proj.weight', 'vision_model.vision_model.encoder.layers.22.self_attn.q_proj.bias', 'vision_model.vision_model.encoder.layers.22.self_attn.q_proj.weight', 'vision_model.vision_model.encoder.layers.22.self_attn.v_proj.bias', 'vision_model.vision_model.encoder.layers.22.self_attn.v_proj.weight', 'vision_model.vision_model.encoder.layers.23.layer_norm1.bias', 'vision_model.vision_model.encoder.layers.23.layer_norm1.weight', 'vision_model.vision_model.encoder.layers.23.layer_norm2.bias', 'vision_model.vision_model.encoder.layers.23.layer_norm2.weight', 'vision_model.vision_model.encoder.layers.23.mlp.fc1.bias', 'vision_model.vision_model.encoder.layers.23.mlp.fc1.weight', 'vision_model.vision_model.encoder.layers.23.mlp.fc2.bias', 'vision_model.vision_model.encoder.layers.23.mlp.fc2.weight', 'vision_model.vision_model.encoder.layers.23.self_attn.k_proj.bias', 'vision_model.vision_model.encoder.layers.23.self_attn.k_proj.weight', 'vision_model.vision_model.encoder.layers.23.self_attn.out_proj.bias', 'vision_model.vision_model.encoder.layers.23.self_attn.out_proj.weight', 'vision_model.vision_model.encoder.layers.23.self_attn.q_proj.bias', 'vision_model.vision_model.encoder.layers.23.self_attn.q_proj.weight', 'vision_model.vision_model.encoder.layers.23.self_attn.v_proj.bias', 'vision_model.vision_model.encoder.layers.23.self_attn.v_proj.weight', 'vision_model.vision_model.encoder.layers.24.layer_norm1.bias', 'vision_model.vision_model.encoder.layers.24.layer_norm1.weight', 'vision_model.vision_model.encoder.layers.24.layer_norm2.bias', 'vision_model.vision_model.encoder.layers.24.layer_norm2.weight', 'vision_model.vision_model.encoder.layers.24.mlp.fc1.bias', 'vision_model.vision_model.encoder.layers.24.mlp.fc1.weight', 'vision_model.vision_model.encoder.layers.24.mlp.fc2.bias', 'vision_model.vision_model.encoder.layers.24.mlp.fc2.weight', 'vision_model.vision_model.encoder.layers.24.self_attn.k_proj.bias', 'vision_model.vision_model.encoder.layers.24.self_attn.k_proj.weight', 'vision_model.vision_model.encoder.layers.24.self_attn.out_proj.bias', 'vision_model.vision_model.encoder.layers.24.self_attn.out_proj.weight', 'vision_model.vision_model.encoder.layers.24.self_attn.q_proj.bias', 'vision_model.vision_model.encoder.layers.24.self_attn.q_proj.weight', 'vision_model.vision_model.encoder.layers.24.self_attn.v_proj.bias', 'vision_model.vision_model.encoder.layers.24.self_attn.v_proj.weight', 'vision_model.vision_model.encoder.layers.25.layer_norm1.bias', 'vision_model.vision_model.encoder.layers.25.layer_norm1.weight', 'vision_model.vision_model.encoder.layers.25.layer_norm2.bias', 'vision_model.vision_model.encoder.layers.25.layer_norm2.weight', 'vision_model.vision_model.encoder.layers.25.mlp.fc1.bias', 'vision_model.vision_model.encoder.layers.25.mlp.fc1.weight', 'vision_model.vision_model.encoder.layers.25.mlp.fc2.bias', 'vision_model.vision_model.encoder.layers.25.mlp.fc2.weight', 'vision_model.vision_model.encoder.layers.25.self_attn.k_proj.bias', 'vision_model.vision_model.encoder.layers.25.self_attn.k_proj.weight', 'vision_model.vision_model.encoder.layers.25.self_attn.out_proj.bias', 'vision_model.vision_model.encoder.layers.25.self_attn.out_proj.weight', 'vision_model.vision_model.encoder.layers.25.self_attn.q_proj.bias', 'vision_model.vision_model.encoder.layers.25.self_attn.q_proj.weight', 'vision_model.vision_model.encoder.layers.25.self_attn.v_proj.bias', 'vision_model.vision_model.encoder.layers.25.self_attn.v_proj.weight', 'vision_model.vision_model.encoder.layers.26.layer_norm1.bias', 'vision_model.vision_model.encoder.layers.26.layer_norm1.weight', 'vision_model.vision_model.encoder.layers.26.layer_norm2.bias', 'vision_model.vision_model.encoder.layers.26.layer_norm2.weight', 'vision_model.vision_model.encoder.layers.26.mlp.fc1.bias', 'vision_model.vision_model.encoder.layers.26.mlp.fc1.weight', 'vision_model.vision_model.encoder.layers.26.mlp.fc2.bias', 'vision_model.vision_model.encoder.layers.26.mlp.fc2.weight', 'vision_model.vision_model.encoder.layers.26.self_attn.k_proj.bias', 'vision_model.vision_model.encoder.layers.26.self_attn.k_proj.weight', 'vision_model.vision_model.encoder.layers.26.self_attn.out_proj.bias', 'vision_model.vision_model.encoder.layers.26.self_attn.out_proj.weight', 'vision_model.vision_model.encoder.layers.26.self_attn.q_proj.bias', 'vision_model.vision_model.encoder.layers.26.self_attn.q_proj.weight', 'vision_model.vision_model.encoder.layers.26.self_attn.v_proj.bias', 'vision_model.vision_model.encoder.layers.26.self_attn.v_proj.weight', 'vision_model.vision_model.encoder.layers.3.layer_norm1.bias', 'vision_model.vision_model.encoder.layers.3.layer_norm1.weight', 'vision_model.vision_model.encoder.layers.3.layer_norm2.bias', 'vision_model.vision_model.encoder.layers.3.layer_norm2.weight', 'vision_model.vision_model.encoder.layers.3.mlp.fc1.bias', 'vision_model.vision_model.encoder.layers.3.mlp.fc1.weight', 'vision_model.vision_model.encoder.layers.3.mlp.fc2.bias', 'vision_model.vision_model.encoder.layers.3.mlp.fc2.weight', 'vision_model.vision_model.encoder.layers.3.self_attn.k_proj.bias', 'vision_model.vision_model.encoder.layers.3.self_attn.k_proj.weight', 'vision_model.vision_model.encoder.layers.3.self_attn.out_proj.bias', 'vision_model.vision_model.encoder.layers.3.self_attn.out_proj.weight', 'vision_model.vision_model.encoder.layers.3.self_attn.q_proj.bias', 'vision_model.vision_model.encoder.layers.3.self_attn.q_proj.weight', 'vision_model.vision_model.encoder.layers.3.self_attn.v_proj.bias', 'vision_model.vision_model.encoder.layers.3.self_attn.v_proj.weight', 'vision_model.vision_model.encoder.layers.4.layer_norm1.bias', 'vision_model.vision_model.encoder.layers.4.layer_norm1.weight', 'vision_model.vision_model.encoder.layers.4.layer_norm2.bias', 'vision_model.vision_model.encoder.layers.4.layer_norm2.weight', 'vision_model.vision_model.encoder.layers.4.mlp.fc1.bias', 'vision_model.vision_model.encoder.layers.4.mlp.fc1.weight', 'vision_model.vision_model.encoder.layers.4.mlp.fc2.bias', 'vision_model.vision_model.encoder.layers.4.mlp.fc2.weight', 'vision_model.vision_model.encoder.layers.4.self_attn.k_proj.bias', 'vision_model.vision_model.encoder.layers.4.self_attn.k_proj.weight', 'vision_model.vision_model.encoder.layers.4.self_attn.out_proj.bias', 'vision_model.vision_model.encoder.layers.4.self_attn.out_proj.weight', 'vision_model.vision_model.encoder.layers.4.self_attn.q_proj.bias', 'vision_model.vision_model.encoder.layers.4.self_attn.q_proj.weight', 'vision_model.vision_model.encoder.layers.4.self_attn.v_proj.bias', 'vision_model.vision_model.encoder.layers.4.self_attn.v_proj.weight', 'vision_model.vision_model.encoder.layers.5.layer_norm1.bias', 'vision_model.vision_model.encoder.layers.5.layer_norm1.weight', 'vision_model.vision_model.encoder.layers.5.layer_norm2.bias', 'vision_model.vision_model.encoder.layers.5.layer_norm2.weight', 'vision_model.vision_model.encoder.layers.5.mlp.fc1.bias', 'vision_model.vision_model.encoder.layers.5.mlp.fc1.weight', 'vision_model.vision_model.encoder.layers.5.mlp.fc2.bias', 'vision_model.vision_model.encoder.layers.5.mlp.fc2.weight', 'vision_model.vision_model.encoder.layers.5.self_attn.k_proj.bias', 'vision_model.vision_model.encoder.layers.5.self_attn.k_proj.weight', 'vision_model.vision_model.encoder.layers.5.self_attn.out_proj.bias', 'vision_model.vision_model.encoder.layers.5.self_attn.out_proj.weight', 'vision_model.vision_model.encoder.layers.5.self_attn.q_proj.bias', 'vision_model.vision_model.encoder.layers.5.self_attn.q_proj.weight', 'vision_model.vision_model.encoder.layers.5.self_attn.v_proj.bias', 'vision_model.vision_model.encoder.layers.5.self_attn.v_proj.weight', 'vision_model.vision_model.encoder.layers.6.layer_norm1.bias', 'vision_model.vision_model.encoder.layers.6.layer_norm1.weight', 'vision_model.vision_model.encoder.layers.6.layer_norm2.bias', 'vision_model.vision_model.encoder.layers.6.layer_norm2.weight', 'vision_model.vision_model.encoder.layers.6.mlp.fc1.bias', 'vision_model.vision_model.encoder.layers.6.mlp.fc1.weight', 'vision_model.vision_model.encoder.layers.6.mlp.fc2.bias', 'vision_model.vision_model.encoder.layers.6.mlp.fc2.weight', 'vision_model.vision_model.encoder.layers.6.self_attn.k_proj.bias', 'vision_model.vision_model.encoder.layers.6.self_attn.k_proj.weight', 'vision_model.vision_model.encoder.layers.6.self_attn.out_proj.bias', 'vision_model.vision_model.encoder.layers.6.self_attn.out_proj.weight', 'vision_model.vision_model.encoder.layers.6.self_attn.q_proj.bias', 'vision_model.vision_model.encoder.layers.6.self_attn.q_proj.weight', 'vision_model.vision_model.encoder.layers.6.self_attn.v_proj.bias', 'vision_model.vision_model.encoder.layers.6.self_attn.v_proj.weight', 'vision_model.vision_model.encoder.layers.7.layer_norm1.bias', 'vision_model.vision_model.encoder.layers.7.layer_norm1.weight', 'vision_model.vision_model.encoder.layers.7.layer_norm2.bias', 'vision_model.vision_model.encoder.layers.7.layer_norm2.weight', 'vision_model.vision_model.encoder.layers.7.mlp.fc1.bias', 'vision_model.vision_model.encoder.layers.7.mlp.fc1.weight', 'vision_model.vision_model.encoder.layers.7.mlp.fc2.bias', 'vision_model.vision_model.encoder.layers.7.mlp.fc2.weight', 'vision_model.vision_model.encoder.layers.7.self_attn.k_proj.bias', 'vision_model.vision_model.encoder.layers.7.self_attn.k_proj.weight', 'vision_model.vision_model.encoder.layers.7.self_attn.out_proj.bias', 'vision_model.vision_model.encoder.layers.7.self_attn.out_proj.weight', 'vision_model.vision_model.encoder.layers.7.self_attn.q_proj.bias', 'vision_model.vision_model.encoder.layers.7.self_attn.q_proj.weight', 'vision_model.vision_model.encoder.layers.7.self_attn.v_proj.bias', 'vision_model.vision_model.encoder.layers.7.self_attn.v_proj.weight', 'vision_model.vision_model.encoder.layers.8.layer_norm1.bias', 'vision_model.vision_model.encoder.layers.8.layer_norm1.weight', 'vision_model.vision_model.encoder.layers.8.layer_norm2.bias', 'vision_model.vision_model.encoder.layers.8.layer_norm2.weight', 'vision_model.vision_model.encoder.layers.8.mlp.fc1.bias', 'vision_model.vision_model.encoder.layers.8.mlp.fc1.weight', 'vision_model.vision_model.encoder.layers.8.mlp.fc2.bias', 'vision_model.vision_model.encoder.layers.8.mlp.fc2.weight', 'vision_model.vision_model.encoder.layers.8.self_attn.k_proj.bias', 'vision_model.vision_model.encoder.layers.8.self_attn.k_proj.weight', 'vision_model.vision_model.encoder.layers.8.self_attn.out_proj.bias', 'vision_model.vision_model.encoder.layers.8.self_attn.out_proj.weight', 'vision_model.vision_model.encoder.layers.8.self_attn.q_proj.bias', 'vision_model.vision_model.encoder.layers.8.self_attn.q_proj.weight', 'vision_model.vision_model.encoder.layers.8.self_attn.v_proj.bias', 'vision_model.vision_model.encoder.layers.8.self_attn.v_proj.weight', 'vision_model.vision_model.encoder.layers.9.layer_norm1.bias', 'vision_model.vision_model.encoder.layers.9.layer_norm1.weight', 'vision_model.vision_model.encoder.layers.9.layer_norm2.bias', 'vision_model.vision_model.encoder.layers.9.layer_norm2.weight', 'vision_model.vision_model.encoder.layers.9.mlp.fc1.bias', 'vision_model.vision_model.encoder.layers.9.mlp.fc1.weight', 'vision_model.vision_model.encoder.layers.9.mlp.fc2.bias', 'vision_model.vision_model.encoder.layers.9.mlp.fc2.weight', 'vision_model.vision_model.encoder.layers.9.self_attn.k_proj.bias', 'vision_model.vision_model.encoder.layers.9.self_attn.k_proj.weight', 'vision_model.vision_model.encoder.layers.9.self_attn.out_proj.bias', 'vision_model.vision_model.encoder.layers.9.self_attn.out_proj.weight', 'vision_model.vision_model.encoder.layers.9.self_attn.q_proj.bias', 'vision_model.vision_model.encoder.layers.9.self_attn.q_proj.weight', 'vision_model.vision_model.encoder.layers.9.self_attn.v_proj.bias', 'vision_model.vision_model.encoder.layers.9.self_attn.v_proj.weight', 'vision_model.vision_model.head.attention.in_proj_bias', 'vision_model.vision_model.head.attention.in_proj_weight', 'vision_model.vision_model.head.attention.out_proj.bias', 'vision_model.vision_model.head.attention.out_proj.weight', 'vision_model.vision_model.head.layernorm.bias', 'vision_model.vision_model.head.layernorm.weight', 'vision_model.vision_model.head.mlp.fc1.bias', 'vision_model.vision_model.head.mlp.fc1.weight', 'vision_model.vision_model.head.mlp.fc2.bias', 'vision_model.vision_model.head.mlp.fc2.weight', 'vision_model.vision_model.head.probe', 'vision_model.vision_model.post_layernorm.bias', 'vision_model.vision_model.post_layernorm.weight']\n",
      "- This IS expected if you are initializing JasperVL from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing JasperVL from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "model_name = \"NovaSearch/jasper_en_vision_language_v1\"\n",
    "model_kwargs = {'device': 'cpu',\n",
    "                'trust_remote_code':True\n",
    "                }\n",
    "hf_emd_model = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eea9f510",
   "metadata": {},
   "outputs": [],
   "source": [
    "textsplitter = SemanticChunker(\n",
    "    hf_emd_model,\n",
    "    breakpoint_threshold_type='standard_deviation',\n",
    "    breakpoint_threshold_amount=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3f4eb29a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\nne of the most important things I didn\\'t understand about the world when I was a child is the degree to which the returns for performance are superlinear. Teachers and coaches implicitly told us the returns were linear. \"You get out,\" I heard a thousand times, \"what you put in.\" They meant well, but this is rarely true. If your product is only half as good as your competitor\\'s, you don\\'t get half as many customers.',\n",
       " \"You get no customers, and you go out of business. It's obviously true that the returns for performance are superlinear in business. Some think this is a flaw of capitalism, and that if we changed the rules it would stop being true. But superlinear returns for performance are a feature of the world, not an artifact of rules we've invented. We see the same pattern in fame, power, military victories, knowledge, and even benefit to humanity. In all of these, the rich get richer. [1]\\n\\nYou can't understand the world without understanding the concept of superlinear returns. And if you're ambitious you definitely should, because this will be the wave you surf on.\",\n",
       " '']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textsplitter.split_text(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c254f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "all_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
